{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"SUMMARY/","text":"Home About Limulator Getting Started Tutorials Lidar Basics Blender System Documentation API Reference","title":"SUMMARY"},{"location":"about/","text":"Overview \u00b6 Lidar technology has been widely used to build 3D surface models. It can use several sensors available to generate user-required data, needless to say, it is both time-consuming and expensive. Limulator is a wake of a new technology tool that builds realistic 3D surface models by simulation. Limulator stands for the combination of the words LiDAR and simulator thus becoming Limulator. Limulator has been developed as a generic solution for numerous simulation tasks, including autonomous driving systems, airborne altimetry, and others. This software combines the models or mathematical equations to replicate the process of LiDAR data generation. Majorly it works on three components: 3D model component, laser component, and LiDAR data generation. Why Limulator? \u00b6 The autonomous vehicle (AV) industry is just spreading its roots to reach the potential height of automation and self-driving vehicles. But it all comes down to the robust training of DL models and data generation. Some of the challenges faced are : A large amount of training data is required to train a deep network. Imbalanced data leads to biases in the model in the training phase. Capturing all possible scenarios in the real world and training the AV model on it is infeasible. A high-fidelity limulator can bridge this gap in the training phase and tackle challenges. Various realistic scenarios can be generated using simulated data. It helps in reducing biases. The lack of real-world diversity can be filled by simulation i.e content gap.The simulated data depicts the real-world characteristics which can overcome the appearance gap. The focus of the limulator : Previous contribution of limulator since 2007: Few publications such as Airborne Altimetric LiDAR. Won ISPRS awards Thousands of data download from limulator for educational and aerial platforms. Chandrayan LLRI simulation - Return waveform and point cloud simulation. Limulator Components \u00b6 There are three major components of a limulator: Terrain component Trajectory component Sensor component The terrain Component involves the formation of 3D static terrain imagery that resembles a real scene. The imagery includes great detailing which enables the possibility of scenario generation. The trajectory component on the other hand is responsible for adding dynamism to the static imagery. This is achieved by considering the acceleration and attitude change of moving objects. The platform component takes user input to narrow down the requirements such as platform type (mobile, stationary, ground-based, aerial), placement of the sensor on the platform, sensor type, orientation, trajectory, and so on. The sensor component emphasizes the type of the sensor i.e LiDAR or camera and takes input on sensor characteristics such as position, orientation, PRF, scan range, FOV, etc. Terrain module \u00b6 The terrain module consists creation of static objects. It has sub-components which include the creation of 3D assets, GIS layer, and DEM. Step 1 - Analysis and quantification of the scene An already existing scene is taken from dales data on which the size of terrain and surface structures are identified. The next procedure is to perform object quantification which involves class identification i.e car, building, traffic light, etc, and dimensions of the objects i.e height, width, etc. Step 2 - Data Preparation Data prepared as follows - - Terrain heatmap and texture - The scene\u2019s footprint and design are created in the file format OSM/ .shp/ Basemap data - Computer-generated architecture (CGA) rules file Step 3 - Modelling in CityEngine A mesh file is created in this step, is created as follows - - Adjusting attributes to create more variations of 3D objects - Mapping and applying textures - A triangulated mesh file is generated and it is exported in .obj file format Trajectory Module \u00b6 Here we will add dynamism to the static objects created in the previous step. Three ways to apply dynamism are behavior modeling + trajectory, Nurbs path (blender), and Behavior modeling (blender). The image on the slide 13 PPT","title":"About Limulator"},{"location":"about/#overview","text":"Lidar technology has been widely used to build 3D surface models. It can use several sensors available to generate user-required data, needless to say, it is both time-consuming and expensive. Limulator is a wake of a new technology tool that builds realistic 3D surface models by simulation. Limulator stands for the combination of the words LiDAR and simulator thus becoming Limulator. Limulator has been developed as a generic solution for numerous simulation tasks, including autonomous driving systems, airborne altimetry, and others. This software combines the models or mathematical equations to replicate the process of LiDAR data generation. Majorly it works on three components: 3D model component, laser component, and LiDAR data generation.","title":"Overview"},{"location":"about/#why-limulator","text":"The autonomous vehicle (AV) industry is just spreading its roots to reach the potential height of automation and self-driving vehicles. But it all comes down to the robust training of DL models and data generation. Some of the challenges faced are : A large amount of training data is required to train a deep network. Imbalanced data leads to biases in the model in the training phase. Capturing all possible scenarios in the real world and training the AV model on it is infeasible. A high-fidelity limulator can bridge this gap in the training phase and tackle challenges. Various realistic scenarios can be generated using simulated data. It helps in reducing biases. The lack of real-world diversity can be filled by simulation i.e content gap.The simulated data depicts the real-world characteristics which can overcome the appearance gap. The focus of the limulator : Previous contribution of limulator since 2007: Few publications such as Airborne Altimetric LiDAR. Won ISPRS awards Thousands of data download from limulator for educational and aerial platforms. Chandrayan LLRI simulation - Return waveform and point cloud simulation.","title":"Why Limulator?"},{"location":"about/#limulator-components","text":"There are three major components of a limulator: Terrain component Trajectory component Sensor component The terrain Component involves the formation of 3D static terrain imagery that resembles a real scene. The imagery includes great detailing which enables the possibility of scenario generation. The trajectory component on the other hand is responsible for adding dynamism to the static imagery. This is achieved by considering the acceleration and attitude change of moving objects. The platform component takes user input to narrow down the requirements such as platform type (mobile, stationary, ground-based, aerial), placement of the sensor on the platform, sensor type, orientation, trajectory, and so on. The sensor component emphasizes the type of the sensor i.e LiDAR or camera and takes input on sensor characteristics such as position, orientation, PRF, scan range, FOV, etc.","title":"Limulator Components"},{"location":"about/#terrain-module","text":"The terrain module consists creation of static objects. It has sub-components which include the creation of 3D assets, GIS layer, and DEM. Step 1 - Analysis and quantification of the scene An already existing scene is taken from dales data on which the size of terrain and surface structures are identified. The next procedure is to perform object quantification which involves class identification i.e car, building, traffic light, etc, and dimensions of the objects i.e height, width, etc. Step 2 - Data Preparation Data prepared as follows - - Terrain heatmap and texture - The scene\u2019s footprint and design are created in the file format OSM/ .shp/ Basemap data - Computer-generated architecture (CGA) rules file Step 3 - Modelling in CityEngine A mesh file is created in this step, is created as follows - - Adjusting attributes to create more variations of 3D objects - Mapping and applying textures - A triangulated mesh file is generated and it is exported in .obj file format","title":"Terrain module"},{"location":"about/#trajectory-module","text":"Here we will add dynamism to the static objects created in the previous step. Three ways to apply dynamism are behavior modeling + trajectory, Nurbs path (blender), and Behavior modeling (blender). The image on the slide 13 PPT","title":"Trajectory Module"},{"location":"api/","text":"Modules.laser \u00b6 laser_sensor \u00b6 Creates a laser sensor object with the provided sensor data Attributes: Name Type Description platform_data dict platform_data sensor_data dict sensor_data start_frame int First frame number to start rendering from end_frame int Last frame number to end rendering at prf int Pulse Repetition Frequency sf int Scanning Frequency rotation_type str Type of rotation csv_export_list list List containing all the intersection points data_accuracy int Set last decimal place upto which all data needs to be rounded off data_columns list List of header names of the exported csv file time_per_horizontal_dev float Time interval in seconds between each deviation deviation_per_frame float Angle of deviation between each frame (in degree) dict_data dict A dictionary containing labels and their respective id's dict_keys list List of label id dict_values list List of label names start_vfov float Start of vertical frame of view end_vfov float End of vertical field of view positive_hfov float Start of horizontal feild of view negative_hfov float End of horizontal feild of view flying_height int Mean flying height for rotating polygon type sensor frame_step int The number of frames to process before exporting hfov float Horizontal Field of view horizontal_deviation_per_pulse float Angle of deviation between each horizontal channel (in degree) laser_list list List containg names of all laserline objects laser_origin_orientation_list list List containing orientation values at each frame laser_step int The number of laser lines to process before exporting number_of_channels int Number of vertical channels number_of_hor_channels int Number of horizontal channels out_file_name str File path of the output file name preinitialized_frame bool preinitialized_frame pulses_per_frame int Number of pulses emitted at each frame rotation_type str Rotation type of the lidar sensor scanner_location list Scanner's location in the platform's local coordinate system [x,y,z] sensor_name str Sensor's name sensor_range_max int Maximum range of sensor sensor_range_min int Minimum range of sensor fixed_error_model list List containing lists of randomly generated error values for x, y and z model_range list List of all range values between sensor_range_max and sensor_range_min range_error_model array range_error_model incidence_angle_error_model array incidence_angle_error_model __init__ ( sensor_data , platform_data ) \u00b6 Initialises all the variables. Calls model accuracy which creates error models for range , incidence and fixed errors . laser_origin_orientation_list is created which provides sensor orientation values (omega, phi and kappa) for each frame. Parameters: Name Type Description Default sensor_data json all parameters of the sensor required platform_data json all platform parameters required all_laserlines_frame_range_iterator ( start , end ) \u00b6 Computes intersections for all channels across a given frame range and exports the data to csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required check_intersection ( graph , origin , direction , distance ) \u00b6 Finds out intersections for a laser vector starting from origin in the direction , upto the given distance . Note It looks for intersection untill a point with zero transparency is found Parameters: Name Type Description Default graph bpy . context Blender scene context required origin mathutils . Vector Location of origin required direction mathutils . Vector Location of direction vector required distance int Maximum range distance required Returns a tuple of the following values Type Description Boolean hit mathutils . Vector location mathutils . Vector normal int index int object list matrix float r float g float b float a float ir create_export_file () \u00b6 It calls export function creates a new csv file with the set data_columns writes in w+ mode. create_sensor_frame () \u00b6 Creates a static frame for a laser sensor depending upon the number of channels in the vertical and horizontal FOV export ( data , mode ) \u00b6 Creates or appends data to a pointcloud.csv file with the given data Parameters: Name Type Description Default data list Data to be exported in list form required mode str Writing mode eg. a+ or w+ required get_fixed_error () \u00b6 Returns a random error value from the fixed error model Returns: Type Description list [error in x, error in y, error in z] get_horizontal_deviation_angles () \u00b6 Gives a list of deviation angles for the different sensor types - 1. Rotating and Osscilating - No deviation 2. Flash - Fixed angular deviation 3. Rotating Polygon - Fixed point spacing on ground Returns: Type Description np . ndarray Numpy list of angles of deviation for each laser line get_incidence_error ( angle ) \u00b6 Returns a error for incidence angle Args: Incidence angle at which the point exists Returns: Type Description np . ndarray List of errors for x, y, z - list(error in x, error in y, error in z) get_label_id ( obj_name ) \u00b6 Returns label id based on the intersected object's name. It looks for object_name in dict_values and returns its id from dict_keys. If object name is not found in the label dictionary, id is set to 999 Parameters: Name Type Description Default obj_name string Object's Name required Returns: Type Description int Label id get_range_error ( range ) \u00b6 Returns a list of random values based on input range Generates random error from a normal distribution with a standard deviation for the given range from range_error_model Parameters: Name Type Description Default range float Range at which the point exists required Returns: Type Description np . ndarray Error in range introduce_error ( ray_origin , incident_ray , location , range , incidence_angle ) \u00b6 Recalculates the location of point of intersection after introducing * fixed errors * range errors * incidence angle errors Parameters: Name Type Description Default ray_origin bpy . context . scene . objects ray origin required incident_ray bpy . context . scene . objects incident ray required location list location required range float range required incidence_angle float incidence angle required Returns: Type Description tuple New x, y, z locations laser_object_group_frame_range_iterator ( start , end , laserline_id ) \u00b6 Computes intersections for a number of channels = laser_step across a given frame range and exports the data to pointcloud.csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required laserline_id int Id of the selected laserline required laserline_currentframe_interaction ( frame , selected_laserline_id ) \u00b6 Computes intersections for a selected laserline on the given frame Returns: [x,y,z,t,label,instance_id,incidence_angle,self.sensor_name,frame,channel_id,r,g,b,a,intensity] lidar_sensor_main () \u00b6 Main class to handle creation of sensor frame and to check intersections model_accuracy () \u00b6 Generates a accuracy model based on the error type orientation_csv_static_not_rotating () \u00b6 Creates a positition orientation list for a stationary non rotating type of scanner orientation_csv_static_o () \u00b6 Generates a list for orientation values for a Osscilating type sensor orientation_csv_static_r () \u00b6 Generates a list for orientation values for a rotating type sensor rotating_polygon_iterator () \u00b6 Iterates through all laserlines and frames - with initializing a set of laserlines and checking interesections for sets of frames Since there are large number of laserlines for a rotating polygon type sensor following modifications are made: All laserlines are processed in batches of the defined laser_step The laserline objects are initialized on demand unlike other senor setups where preinitialized_frame is set to True visulize_sensor_frame () \u00b6 Visulises the sensor frame Modules.camera \u00b6 camera \u00b6 Camera class. It lets create and render images based on the camera paeameters Attributes: Name Type Description camera_data json camera data __init__ ( camera_data , platform_data , sensor_data ) \u00b6 Initialize a Camera class. Get all camera parameters Parameters: Name Type Description Default camera_data json camera data required platform_data json Platform data required sensor_data json Sensor Data required create_camera () \u00b6 Creates a blender camera object inside empty camera object render_frames () \u00b6 Renders frames for the full scene with the selected camera Modules.limulator_platform \u00b6 platform_class \u00b6 Platform class. It lets us add sensors and cameras __init__ ( platform_data , sensor_data ) \u00b6 Initialize and create a platform object in Blender Scene. If a trajectory file is avaialable animates the platform based on the position and orientation at each frame Else it links the platform to a mentioned blender object add_camera ( camera_data ) \u00b6 Creates a camera child object inside platform object add_sensor ( sensor_data ) \u00b6 Creates a sensor child object inside platform object main2 \u00b6 LIST_OT_DeleteItem \u00b6 Bases: Operator Delete the selected item from the list. LIST_OT_DeleteLabelItem \u00b6 Bases: Operator Delete the selected item from the list. LIST_OT_EmptyLabelList \u00b6 Bases: Operator Reevaluates the Label list. LIST_OT_ExportLabelList \u00b6 Bases: Operator Exports the Label list. LIST_OT_MoveItem \u00b6 Bases: Operator Move an item in the list. move_index () \u00b6 Move index of an item render queue while clamping it. LIST_OT_NewItem \u00b6 Bases: Operator Add a new item to the list. LIST_OT_NewLabelItem \u00b6 Bases: Operator Add a new label to list. LabelListItem \u00b6 Bases: PropertyGroup Group of properties representing an label item in the list. ListItem \u00b6 Bases: PropertyGroup Group of properties representing an item in the list. MY_UL_List \u00b6 Bases: UIList Demo UIList. SCENE_LABEL_List \u00b6 Bases: UIList Display Labels createNew ( sensor_count , camera_count ) \u00b6 Creates a empty json structure for a new platform design Args: sensor_count (int) - Count of the lidar sensors to be added camera_count (int) - Count of the camera sensors to be added delete_platforms () \u00b6 Deletes all pre generated sensor objects Returns: None export_json ( override = False ) \u00b6 Exports json file containing all platform, lidar and camera parameters to a given location or overrides a given json file with new data import_label_dict () \u00b6 Imports label dictionary json file from the provided path for label_dict_filepath read_JSON_data () \u00b6 Reads json file containing all platform, lidar and camera parameters to a given location","title":"API Reference"},{"location":"api/#Modules.laser","text":"","title":"laser"},{"location":"api/#Modules.laser.laser_sensor","text":"Creates a laser sensor object with the provided sensor data Attributes: Name Type Description platform_data dict platform_data sensor_data dict sensor_data start_frame int First frame number to start rendering from end_frame int Last frame number to end rendering at prf int Pulse Repetition Frequency sf int Scanning Frequency rotation_type str Type of rotation csv_export_list list List containing all the intersection points data_accuracy int Set last decimal place upto which all data needs to be rounded off data_columns list List of header names of the exported csv file time_per_horizontal_dev float Time interval in seconds between each deviation deviation_per_frame float Angle of deviation between each frame (in degree) dict_data dict A dictionary containing labels and their respective id's dict_keys list List of label id dict_values list List of label names start_vfov float Start of vertical frame of view end_vfov float End of vertical field of view positive_hfov float Start of horizontal feild of view negative_hfov float End of horizontal feild of view flying_height int Mean flying height for rotating polygon type sensor frame_step int The number of frames to process before exporting hfov float Horizontal Field of view horizontal_deviation_per_pulse float Angle of deviation between each horizontal channel (in degree) laser_list list List containg names of all laserline objects laser_origin_orientation_list list List containing orientation values at each frame laser_step int The number of laser lines to process before exporting number_of_channels int Number of vertical channels number_of_hor_channels int Number of horizontal channels out_file_name str File path of the output file name preinitialized_frame bool preinitialized_frame pulses_per_frame int Number of pulses emitted at each frame rotation_type str Rotation type of the lidar sensor scanner_location list Scanner's location in the platform's local coordinate system [x,y,z] sensor_name str Sensor's name sensor_range_max int Maximum range of sensor sensor_range_min int Minimum range of sensor fixed_error_model list List containing lists of randomly generated error values for x, y and z model_range list List of all range values between sensor_range_max and sensor_range_min range_error_model array range_error_model incidence_angle_error_model array incidence_angle_error_model","title":"laser_sensor"},{"location":"api/#Modules.laser.laser_sensor.__init__","text":"Initialises all the variables. Calls model accuracy which creates error models for range , incidence and fixed errors . laser_origin_orientation_list is created which provides sensor orientation values (omega, phi and kappa) for each frame. Parameters: Name Type Description Default sensor_data json all parameters of the sensor required platform_data json all platform parameters required","title":"__init__()"},{"location":"api/#Modules.laser.laser_sensor.all_laserlines_frame_range_iterator","text":"Computes intersections for all channels across a given frame range and exports the data to csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required","title":"all_laserlines_frame_range_iterator()"},{"location":"api/#Modules.laser.laser_sensor.check_intersection","text":"Finds out intersections for a laser vector starting from origin in the direction , upto the given distance . Note It looks for intersection untill a point with zero transparency is found Parameters: Name Type Description Default graph bpy . context Blender scene context required origin mathutils . Vector Location of origin required direction mathutils . Vector Location of direction vector required distance int Maximum range distance required Returns a tuple of the following values Type Description Boolean hit mathutils . Vector location mathutils . Vector normal int index int object list matrix float r float g float b float a float ir","title":"check_intersection()"},{"location":"api/#Modules.laser.laser_sensor.create_export_file","text":"It calls export function creates a new csv file with the set data_columns writes in w+ mode.","title":"create_export_file()"},{"location":"api/#Modules.laser.laser_sensor.create_sensor_frame","text":"Creates a static frame for a laser sensor depending upon the number of channels in the vertical and horizontal FOV","title":"create_sensor_frame()"},{"location":"api/#Modules.laser.laser_sensor.export","text":"Creates or appends data to a pointcloud.csv file with the given data Parameters: Name Type Description Default data list Data to be exported in list form required mode str Writing mode eg. a+ or w+ required","title":"export()"},{"location":"api/#Modules.laser.laser_sensor.get_fixed_error","text":"Returns a random error value from the fixed error model Returns: Type Description list [error in x, error in y, error in z]","title":"get_fixed_error()"},{"location":"api/#Modules.laser.laser_sensor.get_horizontal_deviation_angles","text":"Gives a list of deviation angles for the different sensor types - 1. Rotating and Osscilating - No deviation 2. Flash - Fixed angular deviation 3. Rotating Polygon - Fixed point spacing on ground Returns: Type Description np . ndarray Numpy list of angles of deviation for each laser line","title":"get_horizontal_deviation_angles()"},{"location":"api/#Modules.laser.laser_sensor.get_incidence_error","text":"Returns a error for incidence angle Args: Incidence angle at which the point exists Returns: Type Description np . ndarray List of errors for x, y, z - list(error in x, error in y, error in z)","title":"get_incidence_error()"},{"location":"api/#Modules.laser.laser_sensor.get_label_id","text":"Returns label id based on the intersected object's name. It looks for object_name in dict_values and returns its id from dict_keys. If object name is not found in the label dictionary, id is set to 999 Parameters: Name Type Description Default obj_name string Object's Name required Returns: Type Description int Label id","title":"get_label_id()"},{"location":"api/#Modules.laser.laser_sensor.get_range_error","text":"Returns a list of random values based on input range Generates random error from a normal distribution with a standard deviation for the given range from range_error_model Parameters: Name Type Description Default range float Range at which the point exists required Returns: Type Description np . ndarray Error in range","title":"get_range_error()"},{"location":"api/#Modules.laser.laser_sensor.introduce_error","text":"Recalculates the location of point of intersection after introducing * fixed errors * range errors * incidence angle errors Parameters: Name Type Description Default ray_origin bpy . context . scene . objects ray origin required incident_ray bpy . context . scene . objects incident ray required location list location required range float range required incidence_angle float incidence angle required Returns: Type Description tuple New x, y, z locations","title":"introduce_error()"},{"location":"api/#Modules.laser.laser_sensor.laser_object_group_frame_range_iterator","text":"Computes intersections for a number of channels = laser_step across a given frame range and exports the data to pointcloud.csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required laserline_id int Id of the selected laserline required","title":"laser_object_group_frame_range_iterator()"},{"location":"api/#Modules.laser.laser_sensor.laserline_currentframe_interaction","text":"Computes intersections for a selected laserline on the given frame Returns: [x,y,z,t,label,instance_id,incidence_angle,self.sensor_name,frame,channel_id,r,g,b,a,intensity]","title":"laserline_currentframe_interaction()"},{"location":"api/#Modules.laser.laser_sensor.lidar_sensor_main","text":"Main class to handle creation of sensor frame and to check intersections","title":"lidar_sensor_main()"},{"location":"api/#Modules.laser.laser_sensor.model_accuracy","text":"Generates a accuracy model based on the error type","title":"model_accuracy()"},{"location":"api/#Modules.laser.laser_sensor.orientation_csv_static_not_rotating","text":"Creates a positition orientation list for a stationary non rotating type of scanner","title":"orientation_csv_static_not_rotating()"},{"location":"api/#Modules.laser.laser_sensor.orientation_csv_static_o","text":"Generates a list for orientation values for a Osscilating type sensor","title":"orientation_csv_static_o()"},{"location":"api/#Modules.laser.laser_sensor.orientation_csv_static_r","text":"Generates a list for orientation values for a rotating type sensor","title":"orientation_csv_static_r()"},{"location":"api/#Modules.laser.laser_sensor.rotating_polygon_iterator","text":"Iterates through all laserlines and frames - with initializing a set of laserlines and checking interesections for sets of frames Since there are large number of laserlines for a rotating polygon type sensor following modifications are made: All laserlines are processed in batches of the defined laser_step The laserline objects are initialized on demand unlike other senor setups where preinitialized_frame is set to True","title":"rotating_polygon_iterator()"},{"location":"api/#Modules.laser.laser_sensor.visulize_sensor_frame","text":"Visulises the sensor frame","title":"visulize_sensor_frame()"},{"location":"api/#Modules.camera","text":"","title":"camera"},{"location":"api/#Modules.camera.camera","text":"Camera class. It lets create and render images based on the camera paeameters Attributes: Name Type Description camera_data json camera data","title":"camera"},{"location":"api/#Modules.camera.camera.__init__","text":"Initialize a Camera class. Get all camera parameters Parameters: Name Type Description Default camera_data json camera data required platform_data json Platform data required sensor_data json Sensor Data required","title":"__init__()"},{"location":"api/#Modules.camera.camera.create_camera","text":"Creates a blender camera object inside empty camera object","title":"create_camera()"},{"location":"api/#Modules.camera.camera.render_frames","text":"Renders frames for the full scene with the selected camera","title":"render_frames()"},{"location":"api/#Modules.limulator_platform","text":"","title":"limulator_platform"},{"location":"api/#Modules.limulator_platform.platform_class","text":"Platform class. It lets us add sensors and cameras","title":"platform_class"},{"location":"api/#Modules.limulator_platform.platform_class.__init__","text":"Initialize and create a platform object in Blender Scene. If a trajectory file is avaialable animates the platform based on the position and orientation at each frame Else it links the platform to a mentioned blender object","title":"__init__()"},{"location":"api/#Modules.limulator_platform.platform_class.add_camera","text":"Creates a camera child object inside platform object","title":"add_camera()"},{"location":"api/#Modules.limulator_platform.platform_class.add_sensor","text":"Creates a sensor child object inside platform object","title":"add_sensor()"},{"location":"api/#main2","text":"","title":"main2"},{"location":"api/#main2.LIST_OT_DeleteItem","text":"Bases: Operator Delete the selected item from the list.","title":"LIST_OT_DeleteItem"},{"location":"api/#main2.LIST_OT_DeleteLabelItem","text":"Bases: Operator Delete the selected item from the list.","title":"LIST_OT_DeleteLabelItem"},{"location":"api/#main2.LIST_OT_EmptyLabelList","text":"Bases: Operator Reevaluates the Label list.","title":"LIST_OT_EmptyLabelList"},{"location":"api/#main2.LIST_OT_ExportLabelList","text":"Bases: Operator Exports the Label list.","title":"LIST_OT_ExportLabelList"},{"location":"api/#main2.LIST_OT_MoveItem","text":"Bases: Operator Move an item in the list.","title":"LIST_OT_MoveItem"},{"location":"api/#main2.LIST_OT_MoveItem.move_index","text":"Move index of an item render queue while clamping it.","title":"move_index()"},{"location":"api/#main2.LIST_OT_NewItem","text":"Bases: Operator Add a new item to the list.","title":"LIST_OT_NewItem"},{"location":"api/#main2.LIST_OT_NewLabelItem","text":"Bases: Operator Add a new label to list.","title":"LIST_OT_NewLabelItem"},{"location":"api/#main2.LabelListItem","text":"Bases: PropertyGroup Group of properties representing an label item in the list.","title":"LabelListItem"},{"location":"api/#main2.ListItem","text":"Bases: PropertyGroup Group of properties representing an item in the list.","title":"ListItem"},{"location":"api/#main2.MY_UL_List","text":"Bases: UIList Demo UIList.","title":"MY_UL_List"},{"location":"api/#main2.SCENE_LABEL_List","text":"Bases: UIList Display Labels","title":"SCENE_LABEL_List"},{"location":"api/#main2.createNew","text":"Creates a empty json structure for a new platform design Args: sensor_count (int) - Count of the lidar sensors to be added camera_count (int) - Count of the camera sensors to be added","title":"createNew()"},{"location":"api/#main2.delete_platforms","text":"Deletes all pre generated sensor objects Returns: None","title":"delete_platforms()"},{"location":"api/#main2.export_json","text":"Exports json file containing all platform, lidar and camera parameters to a given location or overrides a given json file with new data","title":"export_json()"},{"location":"api/#main2.import_label_dict","text":"Imports label dictionary json file from the provided path for label_dict_filepath","title":"import_label_dict()"},{"location":"api/#main2.read_JSON_data","text":"Reads json file containing all platform, lidar and camera parameters to a given location","title":"read_JSON_data()"},{"location":"index_1/","text":"Welcome to Limulator Docs \u00b6 Limulator \u00b6 Limulator, i.e. LiDAR + Simulator, is a tool intended to simulate the lidar data generation and labelling process. Since then, it has come a long way to become a platform to actively support the development and testing of autonomous driving systems. As the development of autonomous systems picks pace, it has become absolutely critical to extensively test them in virtual environments before deploying them in public settings. Here we present one solution for that purpose: Limulator, a flexible simulator for point clouds, autonomous driving and much more. Simulator \u00b6 The simulator is built upon Blender, which provides support for the computation of high-fidelity physics, and simulation rendering and controls all the actors. Limulator controls the trajectories of actors, sensors, cameras and platforms by providing a UI interface inside Blender. // Some grahic image explaining connection of blender and limulator To gain a perspective on the structure of the Limulator, some of the major components are listed below: Scene : A scene is a 3D real-world model, generated in a blend file. Platforms : Platforms are all objects, dynamic and static, where sensors and cameras can be embedded. They can be chosen from the scene or imported from outside. Sensors : Sensors are static objects fixed on platforms. Currently, limulator only supports LiDar sensors. Cameras : Common as sensors, cameras are static objects fixed on platforms. Camera support in the limulator is implemented through the Blender camera module. Check out some quick demos. // Demo 1 (Car lidar) // Demo 2 (Car camera) // Demo 3 (Drone lidar) // Demo 4 (Drone camera maybe?)","title":"Welcome to Limulator Docs"},{"location":"index_1/#welcome-to-limulator-docs","text":"","title":"Welcome to Limulator Docs"},{"location":"index_1/#limulator","text":"Limulator, i.e. LiDAR + Simulator, is a tool intended to simulate the lidar data generation and labelling process. Since then, it has come a long way to become a platform to actively support the development and testing of autonomous driving systems. As the development of autonomous systems picks pace, it has become absolutely critical to extensively test them in virtual environments before deploying them in public settings. Here we present one solution for that purpose: Limulator, a flexible simulator for point clouds, autonomous driving and much more.","title":"Limulator"},{"location":"index_1/#simulator","text":"The simulator is built upon Blender, which provides support for the computation of high-fidelity physics, and simulation rendering and controls all the actors. Limulator controls the trajectories of actors, sensors, cameras and platforms by providing a UI interface inside Blender. // Some grahic image explaining connection of blender and limulator To gain a perspective on the structure of the Limulator, some of the major components are listed below: Scene : A scene is a 3D real-world model, generated in a blend file. Platforms : Platforms are all objects, dynamic and static, where sensors and cameras can be embedded. They can be chosen from the scene or imported from outside. Sensors : Sensors are static objects fixed on platforms. Currently, limulator only supports LiDar sensors. Cameras : Common as sensors, cameras are static objects fixed on platforms. Camera support in the limulator is implemented through the Blender camera module. Check out some quick demos. // Demo 1 (Car lidar) // Demo 2 (Car camera) // Demo 3 (Drone lidar) // Demo 4 (Drone camera maybe?)","title":"Simulator"},{"location":"overview/","text":"Limulator \u00b6 Lidar technology has been widely used to build 3D surface models. It can use several sensors available to generate user-required data, needless to say, it is both time-consuming and expensive. Limulator is a wake of a new technology tool that builds realistic 3D surface models by simulation. Limulator stands for the combination of the words LiDAR and simulator thus becoming Limulator. Limulator has been developed as a generic solution for numerous simulation tasks, including autonomous driving systems, airborne altimetry, and others. This software combines the models or mathematical equations to replicate the process of LiDAR data generation. Majorly it works on three components: 3D model component, laser component, and LiDAR data generation. Why Limulator? \u00b6 The autonomous vehicle (AV) industry is just spreading its roots to reach the potential height of automation and self-driving vehicles. But it all comes down to the robust training of DL models and data generation. Some of the challenges faced are : A large amount of training data is required to train a deep network. Imbalanced data leads to biases in the model in the training phase. Capturing all possible scenarios in the real world and training the AV model on it is infeasible. A high-fidelity limulator can bridge this gap in the training phase and tackle challenges. Various realistic scenarios can be generated using simulated data. It helps in reducing biases. The lack of real-world diversity can be filled by simulation i.e content gap.The simulated data depicts the real-world characteristics which can overcome the appearance gap. The focus of the limulator : (Flow chart with 2 levels) Reference 5th slide PPT Previous contribution of limulator since 2007: Few publications such as Airborne Altimetric LiDAR. Won ISPRS awards Thousands of data download from limulator for educational and aerial platforms. Chandrayan LLRI simulation - Return waveform and point cloud simulation. Limulator Components \u00b6 There are four major components of a limulator: Terrain component Trajectory component Platform component Sensor component (Image on 7th slide to be included here) The terrain Component involves the formation of 3D static terrain imagery that resembles a real scene. The imagery includes great detailing which enables the possibility of scenario generation. The trajectory component on the other hand is responsible for adding dynamism to the static imagery. This is achieved by considering the acceleration and attitude change of moving objects. The platform component takes user input to narrow down the requirements such as platform type (mobile, stationary, ground-based, aerial), placement of the sensor on the platform, sensor type, orientation, trajectory, and so on. The sensor component emphasizes the type of the sensor i.e LiDAR or camera and takes input on sensor characteristics such as position, orientation, PRF, scan range, FOV, etc. Terrain module \u00b6 Trajectory Module \u00b6 Simulator / Platform setup \u00b6 - Slide 17 Platform Module \u00b6 Sensor & Camera Module \u00b6 Limulate! \u00b6 Limulator UI \u00b6","title":"Limulator"},{"location":"overview/#limulator","text":"Lidar technology has been widely used to build 3D surface models. It can use several sensors available to generate user-required data, needless to say, it is both time-consuming and expensive. Limulator is a wake of a new technology tool that builds realistic 3D surface models by simulation. Limulator stands for the combination of the words LiDAR and simulator thus becoming Limulator. Limulator has been developed as a generic solution for numerous simulation tasks, including autonomous driving systems, airborne altimetry, and others. This software combines the models or mathematical equations to replicate the process of LiDAR data generation. Majorly it works on three components: 3D model component, laser component, and LiDAR data generation.","title":"Limulator"},{"location":"overview/#why-limulator","text":"The autonomous vehicle (AV) industry is just spreading its roots to reach the potential height of automation and self-driving vehicles. But it all comes down to the robust training of DL models and data generation. Some of the challenges faced are : A large amount of training data is required to train a deep network. Imbalanced data leads to biases in the model in the training phase. Capturing all possible scenarios in the real world and training the AV model on it is infeasible. A high-fidelity limulator can bridge this gap in the training phase and tackle challenges. Various realistic scenarios can be generated using simulated data. It helps in reducing biases. The lack of real-world diversity can be filled by simulation i.e content gap.The simulated data depicts the real-world characteristics which can overcome the appearance gap. The focus of the limulator : (Flow chart with 2 levels) Reference 5th slide PPT Previous contribution of limulator since 2007: Few publications such as Airborne Altimetric LiDAR. Won ISPRS awards Thousands of data download from limulator for educational and aerial platforms. Chandrayan LLRI simulation - Return waveform and point cloud simulation.","title":"Why Limulator?"},{"location":"overview/#limulator-components","text":"There are four major components of a limulator: Terrain component Trajectory component Platform component Sensor component (Image on 7th slide to be included here) The terrain Component involves the formation of 3D static terrain imagery that resembles a real scene. The imagery includes great detailing which enables the possibility of scenario generation. The trajectory component on the other hand is responsible for adding dynamism to the static imagery. This is achieved by considering the acceleration and attitude change of moving objects. The platform component takes user input to narrow down the requirements such as platform type (mobile, stationary, ground-based, aerial), placement of the sensor on the platform, sensor type, orientation, trajectory, and so on. The sensor component emphasizes the type of the sensor i.e LiDAR or camera and takes input on sensor characteristics such as position, orientation, PRF, scan range, FOV, etc.","title":"Limulator Components"},{"location":"overview/#terrain-module","text":"","title":"Terrain module"},{"location":"overview/#trajectory-module","text":"","title":"Trajectory Module"},{"location":"overview/#simulator-platform-setup","text":"- Slide 17","title":"Simulator / Platform setup"},{"location":"overview/#platform-module","text":"","title":"Platform Module"},{"location":"overview/#sensor-camera-module","text":"","title":"Sensor &amp; Camera Module"},{"location":"overview/#limulate","text":"","title":"Limulate!"},{"location":"overview/#limulator-ui","text":"","title":"Limulator UI"},{"location":"references/","text":"Tutorials \u00b6 Basic Flow \u00b6 Scene Generation with Esri City Engine and Blender Platform and Sensor Setup Guides \u00b6 Using external trajectories How to calculate correct frame rate for a sensor How to scale animation to a new frame rate Understanding sensor parameters Lidar Basics \u00b6 Simple Lidar Terminology Know about differnet scanning patterns any lidar Lecture 3 Lecture 4 Lecture 5A Lecture 5B Lidar Principles Blender \u00b6 For Blender Python API documentation visit Blender Python API Keyframing in Blender Create object animation through nurbs path Setup blender preferences to run main2.py automatically CloudCompare \u00b6 Cloudcompare Scalar fields API Reference \u00b6 Laser Module Camera Module Platform Module","title":"Tutorials"},{"location":"references/#tutorials","text":"","title":"Tutorials"},{"location":"references/#basic-flow","text":"Scene Generation with Esri City Engine and Blender Platform and Sensor Setup","title":"Basic Flow"},{"location":"references/#guides","text":"Using external trajectories How to calculate correct frame rate for a sensor How to scale animation to a new frame rate Understanding sensor parameters","title":"Guides"},{"location":"references/#lidar-basics","text":"Simple Lidar Terminology Know about differnet scanning patterns any lidar Lecture 3 Lecture 4 Lecture 5A Lecture 5B Lidar Principles","title":"Lidar Basics"},{"location":"references/#blender","text":"For Blender Python API documentation visit Blender Python API Keyframing in Blender Create object animation through nurbs path Setup blender preferences to run main2.py automatically","title":"Blender"},{"location":"references/#cloudcompare","text":"Cloudcompare Scalar fields","title":"CloudCompare"},{"location":"references/#api-reference","text":"Laser Module Camera Module Platform Module","title":"API Reference"},{"location":"startup/","text":"Getting Started \u00b6 Download \u00b6 With Git \u00b6 Limulator can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/bharatlohani/Limulator---Blender.git From Zip \u00b6 Download and extract from the below link: Source_Code.zip or copy and paste below link https://github.com/bharatlohani/Limulator---Blender/archive/refs/heads/main.zip Above links may not be accessible Limulator is currently being restricted to a selected number of users. To get access to limulator email or contact us at xxx@xxx.com Installation \u00b6 Prerequisites \u00b6 Python (preferably anaconda) Blender (3.1.0 alpha) Visual Studio Code (Only for development) Steps for installation \u00b6 Download the version of blender that matches your system requirements. Follow the instruction manual of blender for more information. Additionally, you need to install python and Visual Studio code on your system. Setting up VScode for blender \u00b6 Head to extensions and install \u201cBlender Development\u201d by Jacques Lucke. Make sure to click the check box of \u2018Reload on save\u2019 and \u2018Allow modify external python\u2019 in the extensions settings. Refer to the image below Follow for more info using-microsoft-visual-studio-code-as-external-ide-for-writing-blender-scripts-add-ons Lastly, you need to install the python extension by Microsoft for visual code studio. First Run \u00b6 Open Blender and open desired .blend file. Go to File > Open > Locate Blender file and click Open If the model is displayed in pink instead of textures. This occurs because blender could not find the texture files. For resolving this we need to provide appropriate location where all texture files are present. Goto File > External Data > Find Missing File Locate the folder where all texture files are present and click Find Missing Files Now we need to run the script file to initiate Limulator addon. For that switch to scripting and goto Text > Open. Locate main2.py file and click Open Text. We can see the progress of the script in the system console. Open the system console, Go to windows > toggle system console, this will open a new window of the system console. Now edit the main2.py file and change the path to the location where all modules are present Run the main2.py file. On the side bar click on Limulator to display all options. Click Click on Platform Parameters button and each sensor. Review the settings and click Ok Click on Visualize to see the sensor Click on Limulate to start the simulation The simulation process is displayed on the console. Once the process is completed a csv file is created at \"Sensor/sensor_name/\". This csv file can be imported in cloud compare to visualize the simulated point cloud.","title":"Getting Started"},{"location":"startup/#getting-started","text":"","title":"Getting Started"},{"location":"startup/#download","text":"","title":"Download"},{"location":"startup/#with-git","text":"Limulator can be directly used from GitHub by cloning the repository into a subfolder of your project root which might be useful if you want to use the very latest version: git clone https://github.com/bharatlohani/Limulator---Blender.git","title":"With Git"},{"location":"startup/#from-zip","text":"Download and extract from the below link: Source_Code.zip or copy and paste below link https://github.com/bharatlohani/Limulator---Blender/archive/refs/heads/main.zip Above links may not be accessible Limulator is currently being restricted to a selected number of users. To get access to limulator email or contact us at xxx@xxx.com","title":"From Zip"},{"location":"startup/#installation","text":"","title":"Installation"},{"location":"startup/#prerequisites","text":"Python (preferably anaconda) Blender (3.1.0 alpha) Visual Studio Code (Only for development)","title":"Prerequisites"},{"location":"startup/#steps-for-installation","text":"Download the version of blender that matches your system requirements. Follow the instruction manual of blender for more information. Additionally, you need to install python and Visual Studio code on your system.","title":"Steps for installation"},{"location":"startup/#setting-up-vscode-for-blender","text":"Head to extensions and install \u201cBlender Development\u201d by Jacques Lucke. Make sure to click the check box of \u2018Reload on save\u2019 and \u2018Allow modify external python\u2019 in the extensions settings. Refer to the image below Follow for more info using-microsoft-visual-studio-code-as-external-ide-for-writing-blender-scripts-add-ons Lastly, you need to install the python extension by Microsoft for visual code studio.","title":"Setting up VScode for blender"},{"location":"startup/#first-run","text":"Open Blender and open desired .blend file. Go to File > Open > Locate Blender file and click Open If the model is displayed in pink instead of textures. This occurs because blender could not find the texture files. For resolving this we need to provide appropriate location where all texture files are present. Goto File > External Data > Find Missing File Locate the folder where all texture files are present and click Find Missing Files Now we need to run the script file to initiate Limulator addon. For that switch to scripting and goto Text > Open. Locate main2.py file and click Open Text. We can see the progress of the script in the system console. Open the system console, Go to windows > toggle system console, this will open a new window of the system console. Now edit the main2.py file and change the path to the location where all modules are present Run the main2.py file. On the side bar click on Limulator to display all options. Click Click on Platform Parameters button and each sensor. Review the settings and click Ok Click on Visualize to see the sensor Click on Limulate to start the simulation The simulation process is displayed on the console. Once the process is completed a csv file is created at \"Sensor/sensor_name/\". This csv file can be imported in cloud compare to visualize the simulated point cloud.","title":"First Run"},{"location":"system_doc/","text":"Blend file preparation The Image on slide 16 to be inserted Simulator / Platform setup \u00b6 - Slide 17 Platform Module \u00b6 Configuration of the platform is taken by the user. The user is expected to input orientation, render period as well as platform movement data source. Platform trajectory is taken through a csv file. Images of slide 18 , 19 and 20 to elaborate platform design Sensor & Camera Module \u00b6 LiDAR sensor setup takes inputs from user with the help of an interactive UI. It takes sensor name, position, orientation, PRF, scan range, horizontal and vertical scanning patterns,and error parameters. Slide 22 LiDAR sensor img Camera sensor setup expects camera name, position, orientation, camera FPS, frame resolution and focal length as inputs. Slide 23 camera sensor img Limulate! \u00b6 The final step of the simulation process is limulating. Once the user clicks on limulate button the progress can be seen on the terminaland point cloud is generated. Slide 25 PPT images Limulator UI \u00b6 Class Diagram \u00b6 Process Diagram \u00b6","title":"System Documentation"},{"location":"system_doc/#simulator-platform-setup","text":"- Slide 17","title":"Simulator / Platform setup"},{"location":"system_doc/#platform-module","text":"Configuration of the platform is taken by the user. The user is expected to input orientation, render period as well as platform movement data source. Platform trajectory is taken through a csv file. Images of slide 18 , 19 and 20 to elaborate platform design","title":"Platform Module"},{"location":"system_doc/#sensor-camera-module","text":"LiDAR sensor setup takes inputs from user with the help of an interactive UI. It takes sensor name, position, orientation, PRF, scan range, horizontal and vertical scanning patterns,and error parameters. Slide 22 LiDAR sensor img Camera sensor setup expects camera name, position, orientation, camera FPS, frame resolution and focal length as inputs. Slide 23 camera sensor img","title":"Sensor &amp; Camera Module"},{"location":"system_doc/#limulate","text":"The final step of the simulation process is limulating. Once the user clicks on limulate button the progress can be seen on the terminaland point cloud is generated. Slide 25 PPT images","title":"Limulate!"},{"location":"system_doc/#limulator-ui","text":"","title":"Limulator UI"},{"location":"system_doc/#class-diagram","text":"","title":"Class Diagram"},{"location":"system_doc/#process-diagram","text":"","title":"Process Diagram"},{"location":"tutorials_2/","text":"Overview \u00b6 Scene Generation with Blender \u00b6 Open the blender and change the name of the already existing collection as \"Input\", create a Platforms \u00b6 Now that the scene is generated, The next step is to add some sensors, but before that, we must specify platform objects. Platform objects are the parent objects where sensors are embedded, e.g. vehicles, drones, etc for scanning purposes. It could be a static object (with no trajectory) or a dynamic object with a trajectory imported from outside, or an actor(a dynamic object inside the scene). First things first a platform object with a trajectory is to be created. Perform the following steps to create a platform object : Step 1 Once you are on the home screen of the blender rename the collection as Input and remove the camera and light hierarchy from it. Add a new collection to root i.e scene collection and name it Sensors . the panel should look like this Image of panel Tip 1 Make sure your collections name starts with a capital letter, otherwise you\u2019ll encounter a logical error during script execution. Step 2 Click on the scripting window, click on open, and go to the directory which has your source code saved i.e main2.py open it, and press run. Additionally, you can also see the progress of the script in the system console. This is an optional step. To open the system console head to windows -> toggle system console, this will open a new window of the system console. Now that your script is running go back to the layout page and click on the arrow situated near the viewpoint arrows (x,y, and z direction arrows). You will notice vertical options such as Tool , View , And Limulator . Head to limulator and open the dropdown menu click on create a platform, and a new pop-up will show up refer to the image below to make sure you\u2019re on track. An image of the design platform pop-up in the \u201ccreate platform\u201d Add the number of sensors, for this tutorial we will be adding 1 in the first field. Proceed by adding the path in the directory output field, this is important as your JSON file will be stored in the directory you specify. Tip 2 - It is advisable to use the same directory for output in which you have your source file saved. Once the directory is specified click on ok . Further new fields will be added to the dropdown following this mainly - * Bulk JSON Process * Platform * Sensors * Test Platform Step 3 Now click on platform from the dropdown, and click on platform parameters . This will lead to yet another pop-up. Go to the movement source and select \u201cBlender Object\u201d. Refer to the image below. Platforms parameter pop-up ss Go to Add -> Empty -> Plain axis SS of Add -> Empty -> Plain axis dropdown A new empty object is created move it into the Cars collection and rename it as Cars . This will be our platform object for this tutorial. Step 4 This step consists of keyframing. Which adds a path to the empty object by providing x,y, and z coordinates to it. Lift the editor above to see the timeline and editor controls. Image of timeline and editor panel For explaining keyframing - Adding a short video with steps of keyframing only Once the keyframing is done you can add Cars as a blender object in the platform parameters. Add values to render start as 0 to 160 below the blender object field, click ok and you\u2019re done. Sensors \u00b6 With platform/platforms ready, now its time to add sensors to the object. The process is very similar to creating platforms; a location and orientation with respect to the platform must be provided. For e.g., if the sensor is placed at the platform's centre, the location must be (0, 0, 0). Remaining parameters like PRF, Scanning frequency, Channels, etc., are generic to the lidars sensors. Refer to the image below to fill out your sensor fields for this tutorial. Image of the sesnor parameters Tip 3 : Make sure you click ok after filling all the fields. Head next to the \"Test Platform\" and click on Visualize . Cameras \u00b6 Adding a camera sensor is just the same as adding a lidar sensor with camera-specific parameters like focal length, iso, etc. Limulate! \u00b6 This is where the magic happens. But before that note, a standard dynamic model in Blender runs at 30fps, but the laser works on 2k-800k Hz. Rendering a simulation at such low fps will throw out almost all the information and will be useless. Hence, the animation is scaled to level it to the sensors. Press Limulate to complete the simulation. The remaining part is straightforward. A laser is created at each frame for every sensor, and intersection with model objects is computed using ray casting. If the calculated distance exceeds the sensor max range, it is discarded; otherwise, we label the attribute and store the location of the intersection point in our point cloud. A csv file is created at the directory you specified earlier, you can see the visualize the output in the cloud compare software. Test Platform \u00b6","title":"Tutorials"},{"location":"tutorials_2/#overview","text":"","title":"Overview"},{"location":"tutorials_2/#scene-generation-with-blender","text":"Open the blender and change the name of the already existing collection as \"Input\", create a","title":"Scene Generation with Blender"},{"location":"tutorials_2/#platforms","text":"Now that the scene is generated, The next step is to add some sensors, but before that, we must specify platform objects. Platform objects are the parent objects where sensors are embedded, e.g. vehicles, drones, etc for scanning purposes. It could be a static object (with no trajectory) or a dynamic object with a trajectory imported from outside, or an actor(a dynamic object inside the scene). First things first a platform object with a trajectory is to be created. Perform the following steps to create a platform object : Step 1 Once you are on the home screen of the blender rename the collection as Input and remove the camera and light hierarchy from it. Add a new collection to root i.e scene collection and name it Sensors . the panel should look like this Image of panel Tip 1 Make sure your collections name starts with a capital letter, otherwise you\u2019ll encounter a logical error during script execution. Step 2 Click on the scripting window, click on open, and go to the directory which has your source code saved i.e main2.py open it, and press run. Additionally, you can also see the progress of the script in the system console. This is an optional step. To open the system console head to windows -> toggle system console, this will open a new window of the system console. Now that your script is running go back to the layout page and click on the arrow situated near the viewpoint arrows (x,y, and z direction arrows). You will notice vertical options such as Tool , View , And Limulator . Head to limulator and open the dropdown menu click on create a platform, and a new pop-up will show up refer to the image below to make sure you\u2019re on track. An image of the design platform pop-up in the \u201ccreate platform\u201d Add the number of sensors, for this tutorial we will be adding 1 in the first field. Proceed by adding the path in the directory output field, this is important as your JSON file will be stored in the directory you specify. Tip 2 - It is advisable to use the same directory for output in which you have your source file saved. Once the directory is specified click on ok . Further new fields will be added to the dropdown following this mainly - * Bulk JSON Process * Platform * Sensors * Test Platform Step 3 Now click on platform from the dropdown, and click on platform parameters . This will lead to yet another pop-up. Go to the movement source and select \u201cBlender Object\u201d. Refer to the image below. Platforms parameter pop-up ss Go to Add -> Empty -> Plain axis SS of Add -> Empty -> Plain axis dropdown A new empty object is created move it into the Cars collection and rename it as Cars . This will be our platform object for this tutorial. Step 4 This step consists of keyframing. Which adds a path to the empty object by providing x,y, and z coordinates to it. Lift the editor above to see the timeline and editor controls. Image of timeline and editor panel For explaining keyframing - Adding a short video with steps of keyframing only Once the keyframing is done you can add Cars as a blender object in the platform parameters. Add values to render start as 0 to 160 below the blender object field, click ok and you\u2019re done.","title":"Platforms"},{"location":"tutorials_2/#sensors","text":"With platform/platforms ready, now its time to add sensors to the object. The process is very similar to creating platforms; a location and orientation with respect to the platform must be provided. For e.g., if the sensor is placed at the platform's centre, the location must be (0, 0, 0). Remaining parameters like PRF, Scanning frequency, Channels, etc., are generic to the lidars sensors. Refer to the image below to fill out your sensor fields for this tutorial. Image of the sesnor parameters Tip 3 : Make sure you click ok after filling all the fields. Head next to the \"Test Platform\" and click on Visualize .","title":"Sensors"},{"location":"tutorials_2/#cameras","text":"Adding a camera sensor is just the same as adding a lidar sensor with camera-specific parameters like focal length, iso, etc.","title":"Cameras"},{"location":"tutorials_2/#limulate","text":"This is where the magic happens. But before that note, a standard dynamic model in Blender runs at 30fps, but the laser works on 2k-800k Hz. Rendering a simulation at such low fps will throw out almost all the information and will be useless. Hence, the animation is scaled to level it to the sensors. Press Limulate to complete the simulation. The remaining part is straightforward. A laser is created at each frame for every sensor, and intersection with model objects is computed using ray casting. If the calculated distance exceeds the sensor max range, it is discarded; otherwise, we label the attribute and store the location of the intersection point in our point cloud. A csv file is created at the directory you specified earlier, you can see the visualize the output in the cloud compare software.","title":"Limulate!"},{"location":"tutorials_2/#test-platform","text":"","title":"Test Platform"},{"location":"video/","text":"Video example \u00b6 Lorem ipsum dolor sit amet","title":"Video example"},{"location":"video/#video-example","text":"Lorem ipsum dolor sit amet","title":"Video example"},{"location":"modules/camera/","text":"Camera Module \u00b6 Modules.camera \u00b6 camera \u00b6 Camera class. It lets create and render images based on the camera paeameters Attributes: Name Type Description camera_data json camera data __init__ ( camera_data , platform_data , sensor_data ) \u00b6 Initialize a Camera class. Get all camera parameters Parameters: Name Type Description Default camera_data json camera data required platform_data json Platform data required sensor_data json Sensor Data required create_camera () \u00b6 Creates a blender camera object inside empty camera object render_frames () \u00b6 Renders frames for the full scene with the selected camera","title":"Camera"},{"location":"modules/camera/#camera-module","text":"","title":"Camera Module"},{"location":"modules/camera/#Modules.camera","text":"","title":"camera"},{"location":"modules/camera/#Modules.camera.camera","text":"Camera class. It lets create and render images based on the camera paeameters Attributes: Name Type Description camera_data json camera data","title":"camera"},{"location":"modules/camera/#Modules.camera.camera.__init__","text":"Initialize a Camera class. Get all camera parameters Parameters: Name Type Description Default camera_data json camera data required platform_data json Platform data required sensor_data json Sensor Data required","title":"__init__()"},{"location":"modules/camera/#Modules.camera.camera.create_camera","text":"Creates a blender camera object inside empty camera object","title":"create_camera()"},{"location":"modules/camera/#Modules.camera.camera.render_frames","text":"Renders frames for the full scene with the selected camera","title":"render_frames()"},{"location":"modules/laser/","text":"Laser Module \u00b6 Modules.laser \u00b6 laser_sensor \u00b6 Creates a laser sensor object with the provided sensor data Attributes: Name Type Description platform_data dict platform_data sensor_data dict sensor_data start_frame int First frame number to start rendering from end_frame int Last frame number to end rendering at prf int Pulse Repetition Frequency sf int Scanning Frequency rotation_type str Type of rotation csv_export_list list List containing all the intersection points data_accuracy int Set last decimal place upto which all data needs to be rounded off data_columns list List of header names of the exported csv file time_per_horizontal_dev float Time interval in seconds between each deviation deviation_per_frame float Angle of deviation between each frame (in degree) dict_data dict A dictionary containing labels and their respective id's dict_keys list List of label id dict_values list List of label names start_vfov float Start of vertical frame of view end_vfov float End of vertical field of view positive_hfov float Start of horizontal feild of view negative_hfov float End of horizontal feild of view flying_height int Mean flying height for rotating polygon type sensor frame_step int The number of frames to process before exporting hfov float Horizontal Field of view horizontal_deviation_per_pulse float Angle of deviation between each horizontal channel (in degree) laser_list list List containg names of all laserline objects laser_origin_orientation_list list List containing orientation values at each frame laser_step int The number of laser lines to process before exporting number_of_channels int Number of vertical channels number_of_hor_channels int Number of horizontal channels out_file_name str File path of the output file name preinitialized_frame bool preinitialized_frame pulses_per_frame int Number of pulses emitted at each frame rotation_type str Rotation type of the lidar sensor scanner_location list Scanner's location in the platform's local coordinate system [x,y,z] sensor_name str Sensor's name sensor_range_max int Maximum range of sensor sensor_range_min int Minimum range of sensor fixed_error_model list List containing lists of randomly generated error values for x, y and z model_range list List of all range values between sensor_range_max and sensor_range_min range_error_model array range_error_model incidence_angle_error_model array incidence_angle_error_model __init__ ( sensor_data , platform_data ) \u00b6 Initialises all the variables. Calls model accuracy which creates error models for range , incidence and fixed errors . laser_origin_orientation_list is created which provides sensor orientation values (omega, phi and kappa) for each frame. Parameters: Name Type Description Default sensor_data json all parameters of the sensor required platform_data json all platform parameters required all_laserlines_frame_range_iterator ( start , end ) \u00b6 Computes intersections for all channels across a given frame range and exports the data to csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required check_intersection ( graph , origin , direction , distance ) \u00b6 Finds out intersections for a laser vector starting from origin in the direction , upto the given distance . Note It looks for intersection untill a point with zero transparency is found Parameters: Name Type Description Default graph bpy . context Blender scene context required origin mathutils . Vector Location of origin required direction mathutils . Vector Location of direction vector required distance int Maximum range distance required Returns a tuple of the following values Type Description Boolean hit mathutils . Vector location mathutils . Vector normal int index int object list matrix float r float g float b float a float ir create_export_file () \u00b6 It calls export function creates a new csv file with the set data_columns writes in w+ mode. create_sensor_frame () \u00b6 Creates a static frame for a laser sensor depending upon the number of channels in the vertical and horizontal FOV export ( data , mode ) \u00b6 Creates or appends data to a pointcloud.csv file with the given data Parameters: Name Type Description Default data list Data to be exported in list form required mode str Writing mode eg. a+ or w+ required get_fixed_error () \u00b6 Returns a random error value from the fixed error model Returns: Type Description list [error in x, error in y, error in z] get_horizontal_deviation_angles () \u00b6 Gives a list of deviation angles for the different sensor types - 1. Rotating and Osscilating - No deviation 2. Flash - Fixed angular deviation 3. Rotating Polygon - Fixed point spacing on ground Returns: Type Description np . ndarray Numpy list of angles of deviation for each laser line get_incidence_error ( angle ) \u00b6 Returns a error for incidence angle Args: Incidence angle at which the point exists Returns: Type Description np . ndarray List of errors for x, y, z - list(error in x, error in y, error in z) get_label_id ( obj_name ) \u00b6 Returns label id based on the intersected object's name. It looks for object_name in dict_values and returns its id from dict_keys. If object name is not found in the label dictionary, id is set to 999 Parameters: Name Type Description Default obj_name string Object's Name required Returns: Type Description int Label id get_range_error ( range ) \u00b6 Returns a list of random values based on input range Generates random error from a normal distribution with a standard deviation for the given range from range_error_model Parameters: Name Type Description Default range float Range at which the point exists required Returns: Type Description np . ndarray Error in range introduce_error ( ray_origin , incident_ray , location , range , incidence_angle ) \u00b6 Recalculates the location of point of intersection after introducing * fixed errors * range errors * incidence angle errors Parameters: Name Type Description Default ray_origin bpy . context . scene . objects ray origin required incident_ray bpy . context . scene . objects incident ray required location list location required range float range required incidence_angle float incidence angle required Returns: Type Description tuple New x, y, z locations laser_object_group_frame_range_iterator ( start , end , laserline_id ) \u00b6 Computes intersections for a number of channels = laser_step across a given frame range and exports the data to pointcloud.csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required laserline_id int Id of the selected laserline required laserline_currentframe_interaction ( frame , selected_laserline_id ) \u00b6 Computes intersections for a selected laserline on the given frame Returns: [x,y,z,t,label,instance_id,incidence_angle,self.sensor_name,frame,channel_id,r,g,b,a,intensity] lidar_sensor_main () \u00b6 Main class to handle creation of sensor frame and to check intersections model_accuracy () \u00b6 Generates a accuracy model based on the error type orientation_csv_static_not_rotating () \u00b6 Creates a positition orientation list for a stationary non rotating type of scanner orientation_csv_static_o () \u00b6 Generates a list for orientation values for a Osscilating type sensor orientation_csv_static_r () \u00b6 Generates a list for orientation values for a rotating type sensor rotating_polygon_iterator () \u00b6 Iterates through all laserlines and frames - with initializing a set of laserlines and checking interesections for sets of frames Since there are large number of laserlines for a rotating polygon type sensor following modifications are made: All laserlines are processed in batches of the defined laser_step The laserline objects are initialized on demand unlike other senor setups where preinitialized_frame is set to True visulize_sensor_frame () \u00b6 Visulises the sensor frame","title":"Laser Module"},{"location":"modules/laser/#laser-module","text":"","title":"Laser Module"},{"location":"modules/laser/#Modules.laser","text":"","title":"laser"},{"location":"modules/laser/#Modules.laser.laser_sensor","text":"Creates a laser sensor object with the provided sensor data Attributes: Name Type Description platform_data dict platform_data sensor_data dict sensor_data start_frame int First frame number to start rendering from end_frame int Last frame number to end rendering at prf int Pulse Repetition Frequency sf int Scanning Frequency rotation_type str Type of rotation csv_export_list list List containing all the intersection points data_accuracy int Set last decimal place upto which all data needs to be rounded off data_columns list List of header names of the exported csv file time_per_horizontal_dev float Time interval in seconds between each deviation deviation_per_frame float Angle of deviation between each frame (in degree) dict_data dict A dictionary containing labels and their respective id's dict_keys list List of label id dict_values list List of label names start_vfov float Start of vertical frame of view end_vfov float End of vertical field of view positive_hfov float Start of horizontal feild of view negative_hfov float End of horizontal feild of view flying_height int Mean flying height for rotating polygon type sensor frame_step int The number of frames to process before exporting hfov float Horizontal Field of view horizontal_deviation_per_pulse float Angle of deviation between each horizontal channel (in degree) laser_list list List containg names of all laserline objects laser_origin_orientation_list list List containing orientation values at each frame laser_step int The number of laser lines to process before exporting number_of_channels int Number of vertical channels number_of_hor_channels int Number of horizontal channels out_file_name str File path of the output file name preinitialized_frame bool preinitialized_frame pulses_per_frame int Number of pulses emitted at each frame rotation_type str Rotation type of the lidar sensor scanner_location list Scanner's location in the platform's local coordinate system [x,y,z] sensor_name str Sensor's name sensor_range_max int Maximum range of sensor sensor_range_min int Minimum range of sensor fixed_error_model list List containing lists of randomly generated error values for x, y and z model_range list List of all range values between sensor_range_max and sensor_range_min range_error_model array range_error_model incidence_angle_error_model array incidence_angle_error_model","title":"laser_sensor"},{"location":"modules/laser/#Modules.laser.laser_sensor.__init__","text":"Initialises all the variables. Calls model accuracy which creates error models for range , incidence and fixed errors . laser_origin_orientation_list is created which provides sensor orientation values (omega, phi and kappa) for each frame. Parameters: Name Type Description Default sensor_data json all parameters of the sensor required platform_data json all platform parameters required","title":"__init__()"},{"location":"modules/laser/#Modules.laser.laser_sensor.all_laserlines_frame_range_iterator","text":"Computes intersections for all channels across a given frame range and exports the data to csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required","title":"all_laserlines_frame_range_iterator()"},{"location":"modules/laser/#Modules.laser.laser_sensor.check_intersection","text":"Finds out intersections for a laser vector starting from origin in the direction , upto the given distance . Note It looks for intersection untill a point with zero transparency is found Parameters: Name Type Description Default graph bpy . context Blender scene context required origin mathutils . Vector Location of origin required direction mathutils . Vector Location of direction vector required distance int Maximum range distance required Returns a tuple of the following values Type Description Boolean hit mathutils . Vector location mathutils . Vector normal int index int object list matrix float r float g float b float a float ir","title":"check_intersection()"},{"location":"modules/laser/#Modules.laser.laser_sensor.create_export_file","text":"It calls export function creates a new csv file with the set data_columns writes in w+ mode.","title":"create_export_file()"},{"location":"modules/laser/#Modules.laser.laser_sensor.create_sensor_frame","text":"Creates a static frame for a laser sensor depending upon the number of channels in the vertical and horizontal FOV","title":"create_sensor_frame()"},{"location":"modules/laser/#Modules.laser.laser_sensor.export","text":"Creates or appends data to a pointcloud.csv file with the given data Parameters: Name Type Description Default data list Data to be exported in list form required mode str Writing mode eg. a+ or w+ required","title":"export()"},{"location":"modules/laser/#Modules.laser.laser_sensor.get_fixed_error","text":"Returns a random error value from the fixed error model Returns: Type Description list [error in x, error in y, error in z]","title":"get_fixed_error()"},{"location":"modules/laser/#Modules.laser.laser_sensor.get_horizontal_deviation_angles","text":"Gives a list of deviation angles for the different sensor types - 1. Rotating and Osscilating - No deviation 2. Flash - Fixed angular deviation 3. Rotating Polygon - Fixed point spacing on ground Returns: Type Description np . ndarray Numpy list of angles of deviation for each laser line","title":"get_horizontal_deviation_angles()"},{"location":"modules/laser/#Modules.laser.laser_sensor.get_incidence_error","text":"Returns a error for incidence angle Args: Incidence angle at which the point exists Returns: Type Description np . ndarray List of errors for x, y, z - list(error in x, error in y, error in z)","title":"get_incidence_error()"},{"location":"modules/laser/#Modules.laser.laser_sensor.get_label_id","text":"Returns label id based on the intersected object's name. It looks for object_name in dict_values and returns its id from dict_keys. If object name is not found in the label dictionary, id is set to 999 Parameters: Name Type Description Default obj_name string Object's Name required Returns: Type Description int Label id","title":"get_label_id()"},{"location":"modules/laser/#Modules.laser.laser_sensor.get_range_error","text":"Returns a list of random values based on input range Generates random error from a normal distribution with a standard deviation for the given range from range_error_model Parameters: Name Type Description Default range float Range at which the point exists required Returns: Type Description np . ndarray Error in range","title":"get_range_error()"},{"location":"modules/laser/#Modules.laser.laser_sensor.introduce_error","text":"Recalculates the location of point of intersection after introducing * fixed errors * range errors * incidence angle errors Parameters: Name Type Description Default ray_origin bpy . context . scene . objects ray origin required incident_ray bpy . context . scene . objects incident ray required location list location required range float range required incidence_angle float incidence angle required Returns: Type Description tuple New x, y, z locations","title":"introduce_error()"},{"location":"modules/laser/#Modules.laser.laser_sensor.laser_object_group_frame_range_iterator","text":"Computes intersections for a number of channels = laser_step across a given frame range and exports the data to pointcloud.csv file Parameters: Name Type Description Default start int Start frame required end int End Frame required laserline_id int Id of the selected laserline required","title":"laser_object_group_frame_range_iterator()"},{"location":"modules/laser/#Modules.laser.laser_sensor.laserline_currentframe_interaction","text":"Computes intersections for a selected laserline on the given frame Returns: [x,y,z,t,label,instance_id,incidence_angle,self.sensor_name,frame,channel_id,r,g,b,a,intensity]","title":"laserline_currentframe_interaction()"},{"location":"modules/laser/#Modules.laser.laser_sensor.lidar_sensor_main","text":"Main class to handle creation of sensor frame and to check intersections","title":"lidar_sensor_main()"},{"location":"modules/laser/#Modules.laser.laser_sensor.model_accuracy","text":"Generates a accuracy model based on the error type","title":"model_accuracy()"},{"location":"modules/laser/#Modules.laser.laser_sensor.orientation_csv_static_not_rotating","text":"Creates a positition orientation list for a stationary non rotating type of scanner","title":"orientation_csv_static_not_rotating()"},{"location":"modules/laser/#Modules.laser.laser_sensor.orientation_csv_static_o","text":"Generates a list for orientation values for a Osscilating type sensor","title":"orientation_csv_static_o()"},{"location":"modules/laser/#Modules.laser.laser_sensor.orientation_csv_static_r","text":"Generates a list for orientation values for a rotating type sensor","title":"orientation_csv_static_r()"},{"location":"modules/laser/#Modules.laser.laser_sensor.rotating_polygon_iterator","text":"Iterates through all laserlines and frames - with initializing a set of laserlines and checking interesections for sets of frames Since there are large number of laserlines for a rotating polygon type sensor following modifications are made: All laserlines are processed in batches of the defined laser_step The laserline objects are initialized on demand unlike other senor setups where preinitialized_frame is set to True","title":"rotating_polygon_iterator()"},{"location":"modules/laser/#Modules.laser.laser_sensor.visulize_sensor_frame","text":"Visulises the sensor frame","title":"visulize_sensor_frame()"},{"location":"modules/others/","text":"Others \u00b6 Modules.global_data \u00b6 Modules.logger \u00b6","title":"Others"},{"location":"modules/others/#others","text":"","title":"Others"},{"location":"modules/others/#Modules.global_data","text":"","title":"global_data"},{"location":"modules/others/#Modules.logger","text":"","title":"logger"},{"location":"modules/platform/","text":"Platform Module \u00b6 Modules.limulator_platform \u00b6 platform_class \u00b6 Platform class. It lets us add sensors and cameras __init__ ( platform_data , sensor_data ) \u00b6 Initialize and create a platform object in Blender Scene. If a trajectory file is avaialable animates the platform based on the position and orientation at each frame Else it links the platform to a mentioned blender object add_camera ( camera_data ) \u00b6 Creates a camera child object inside platform object add_sensor ( sensor_data ) \u00b6 Creates a sensor child object inside platform object","title":"Platform Module"},{"location":"modules/platform/#platform-module","text":"","title":"Platform Module"},{"location":"modules/platform/#Modules.limulator_platform","text":"","title":"limulator_platform"},{"location":"modules/platform/#Modules.limulator_platform.platform_class","text":"Platform class. It lets us add sensors and cameras","title":"platform_class"},{"location":"modules/platform/#Modules.limulator_platform.platform_class.__init__","text":"Initialize and create a platform object in Blender Scene. If a trajectory file is avaialable animates the platform based on the position and orientation at each frame Else it links the platform to a mentioned blender object","title":"__init__()"},{"location":"modules/platform/#Modules.limulator_platform.platform_class.add_camera","text":"Creates a camera child object inside platform object","title":"add_camera()"},{"location":"modules/platform/#Modules.limulator_platform.platform_class.add_sensor","text":"Creates a sensor child object inside platform object","title":"add_sensor()"},{"location":"modules/server_processing/","text":"Server Processing \u00b6 server_processing \u00b6 Modules.multiprocess_script \u00b6","title":"Server Processing"},{"location":"modules/server_processing/#server-processing","text":"","title":"Server Processing"},{"location":"modules/server_processing/#server_processing","text":"","title":"server_processing"},{"location":"modules/server_processing/#Modules.multiprocess_script","text":"","title":"multiprocess_script"},{"location":"modules/ui/","text":"UI Module \u00b6 main2 \u00b6 LIST_OT_DeleteItem \u00b6 Bases: Operator Delete the selected item from the list. LIST_OT_DeleteLabelItem \u00b6 Bases: Operator Delete the selected item from the list. LIST_OT_EmptyLabelList \u00b6 Bases: Operator Reevaluates the Label list. LIST_OT_ExportLabelList \u00b6 Bases: Operator Exports the Label list. LIST_OT_MoveItem \u00b6 Bases: Operator Move an item in the list. move_index () \u00b6 Move index of an item render queue while clamping it. LIST_OT_NewItem \u00b6 Bases: Operator Add a new item to the list. LIST_OT_NewLabelItem \u00b6 Bases: Operator Add a new label to list. LabelListItem \u00b6 Bases: PropertyGroup Group of properties representing an label item in the list. ListItem \u00b6 Bases: PropertyGroup Group of properties representing an item in the list. MY_UL_List \u00b6 Bases: UIList Demo UIList. SCENE_LABEL_List \u00b6 Bases: UIList Display Labels createNew ( sensor_count , camera_count ) \u00b6 Creates a empty json structure for a new platform design Args: sensor_count (int) - Count of the lidar sensors to be added camera_count (int) - Count of the camera sensors to be added delete_platforms () \u00b6 Deletes all pre generated sensor objects Returns: None export_json ( override = False ) \u00b6 Exports json file containing all platform, lidar and camera parameters to a given location or overrides a given json file with new data import_label_dict () \u00b6 Imports label dictionary json file from the provided path for label_dict_filepath read_JSON_data () \u00b6 Reads json file containing all platform, lidar and camera parameters to a given location","title":"UI Module"},{"location":"modules/ui/#ui-module","text":"","title":"UI Module"},{"location":"modules/ui/#main2","text":"","title":"main2"},{"location":"modules/ui/#main2.LIST_OT_DeleteItem","text":"Bases: Operator Delete the selected item from the list.","title":"LIST_OT_DeleteItem"},{"location":"modules/ui/#main2.LIST_OT_DeleteLabelItem","text":"Bases: Operator Delete the selected item from the list.","title":"LIST_OT_DeleteLabelItem"},{"location":"modules/ui/#main2.LIST_OT_EmptyLabelList","text":"Bases: Operator Reevaluates the Label list.","title":"LIST_OT_EmptyLabelList"},{"location":"modules/ui/#main2.LIST_OT_ExportLabelList","text":"Bases: Operator Exports the Label list.","title":"LIST_OT_ExportLabelList"},{"location":"modules/ui/#main2.LIST_OT_MoveItem","text":"Bases: Operator Move an item in the list.","title":"LIST_OT_MoveItem"},{"location":"modules/ui/#main2.LIST_OT_MoveItem.move_index","text":"Move index of an item render queue while clamping it.","title":"move_index()"},{"location":"modules/ui/#main2.LIST_OT_NewItem","text":"Bases: Operator Add a new item to the list.","title":"LIST_OT_NewItem"},{"location":"modules/ui/#main2.LIST_OT_NewLabelItem","text":"Bases: Operator Add a new label to list.","title":"LIST_OT_NewLabelItem"},{"location":"modules/ui/#main2.LabelListItem","text":"Bases: PropertyGroup Group of properties representing an label item in the list.","title":"LabelListItem"},{"location":"modules/ui/#main2.ListItem","text":"Bases: PropertyGroup Group of properties representing an item in the list.","title":"ListItem"},{"location":"modules/ui/#main2.MY_UL_List","text":"Bases: UIList Demo UIList.","title":"MY_UL_List"},{"location":"modules/ui/#main2.SCENE_LABEL_List","text":"Bases: UIList Display Labels","title":"SCENE_LABEL_List"},{"location":"modules/ui/#main2.createNew","text":"Creates a empty json structure for a new platform design Args: sensor_count (int) - Count of the lidar sensors to be added camera_count (int) - Count of the camera sensors to be added","title":"createNew()"},{"location":"modules/ui/#main2.delete_platforms","text":"Deletes all pre generated sensor objects Returns: None","title":"delete_platforms()"},{"location":"modules/ui/#main2.export_json","text":"Exports json file containing all platform, lidar and camera parameters to a given location or overrides a given json file with new data","title":"export_json()"},{"location":"modules/ui/#main2.import_label_dict","text":"Imports label dictionary json file from the provided path for label_dict_filepath","title":"import_label_dict()"},{"location":"modules/ui/#main2.read_JSON_data","text":"Reads json file containing all platform, lidar and camera parameters to a given location","title":"read_JSON_data()"},{"location":"tutorials/animation_scaling/","text":"Scaling Animation in blender \u00b6 Follow below steps to scale animations with the calculated animation scale","title":"How to scale animations by a scaling factor"},{"location":"tutorials/animation_scaling/#scaling-animation-in-blender","text":"Follow below steps to scale animations with the calculated animation scale","title":"Scaling Animation in blender"},{"location":"tutorials/external_traj/","text":"The movement of platform object can also be set up using an external trajectory file. The trajectory is a set of comma separated values of the below values: * Location x: X coordinate of the position of platform at time instant t * Location y: Y coordinate of the position of platform at time instant t * Location z: Z coordinate of the position of platform at time instant t * Orientation omega: Rotation about X axis at time instant t * Orientation phi: Rotation about Y axis at time instant t * Orientation kappa: Rotation about Z axis at time instant t * time instance (t): Time calculated from the start of simulation. The above values must be provided for each time instance at which the laser pulses are being sent out. If comparing to the blender animation the values must be provided for each frame. These values can be caluclated through the Trajectory_for_Lidar3d.py script. We need to provide below parameters in a json file for it: a list of Waypoints: Point at which the platform should pass through at any given time instance Platform_dynamics: velocity Deviation in Acceleration Attitude_deviation in terms of values of roll, pitch, yaw","title":"Using external trajectories for sensors"},{"location":"tutorials/frame_rate_calculation/","text":"Since typical animations are built on frame rates of around 35-30 FPS, which works well with camera sensors. But for lidar sensors works at a much higher frequency and using such low frame rates causes loss of fidelity in the simulated data as the motion between the frames is lost. So we suggest the users to scale the animations to a frame rate which matches the sensor's frequency. Step 1 Calculate Frames per Second (FPS) \u00b6 FPS = PRF/N where: * Number of channels(N): Number of beams in vertical and horizontal direction * Pulse Repetition Frequency (PRF) = Pulses emitted by the sensor in a second Step 2 Calculate Animation Scaling factor (Scale X) \u00b6 Scale X = (New calculated FPS)/(Original FPS) where: * New calculated FPS: Calcualated frame rate using the above formula * Original FPS: Initial FPS of the animation in the blend file Demo: Velodyne 32 LiDAR \u00b6 Number of channels = 32 PRF = 720000 FPS = (720000/32) = 22500 Initial FPS = 25 Final FPS = 22500 Scaling factor = 22500/25 = 900","title":"How to calculate frame rate for a Lidar Sensor"},{"location":"tutorials/frame_rate_calculation/#step-1-calculate-frames-per-second-fps","text":"FPS = PRF/N where: * Number of channels(N): Number of beams in vertical and horizontal direction * Pulse Repetition Frequency (PRF) = Pulses emitted by the sensor in a second","title":"Step 1 Calculate Frames per Second (FPS)"},{"location":"tutorials/frame_rate_calculation/#step-2-calculate-animation-scaling-factor-scale-x","text":"Scale X = (New calculated FPS)/(Original FPS) where: * New calculated FPS: Calcualated frame rate using the above formula * Original FPS: Initial FPS of the animation in the blend file","title":"Step 2 Calculate Animation Scaling factor (Scale X)"},{"location":"tutorials/frame_rate_calculation/#demo-velodyne-32-lidar","text":"Number of channels = 32 PRF = 720000 FPS = (720000/32) = 22500 Initial FPS = 25 Final FPS = 22500 Scaling factor = 22500/25 = 900","title":"Demo: Velodyne 32 LiDAR"},{"location":"tutorials/platform_sensor_setup/","text":"Platform and Sensor Setup \u00b6 Now that the scene is generated, the next step is to create a platform with the desired sensors and cameras. For that we must specify platform objects and what trajectory they would follow. Platform objects are the parent objects where sensors are embedded, e.g. vehicles, drones, etc for scanning purposes. It could be a static object (with no trajectory) or a dynamic object with a trajectory imported from outside, or an actor(a dynamic object inside the scene). First things first a platform object with a trajectory is to be created. Perform the following steps to create a platform object : Prepare Scene Collection \u00b6 Once you are on the home screen of the blender rename the collection as Input and remove the camera and light hierarchy from it. Add a new collection to root i.e scene collection and name it Sensor . The panel should look like this: Collection name is case-sensitive. Make sure your collections name starts with a capital letter, otherwise you\u2019ll encounter a logical error during script execution. Create a platform object \u00b6 Platform objects can be controlled by either of 2 ways: External trajectories Keyframing in Blender For this tutorial we will create a platform object through keyframing: More Information on use of external trajectories Guide: Using external trajectories Create a new empty object. Go to Add > Empty > Plain axis Create a new collection inside Input collection and rename it to Cars Move the empty object into Cars collection Now if needed we can add some dynamic movement to the platform. This step consists of keyframing which adds motion to the empty object by providing location and orientation values. Lift the editor above to see the timeline and editor controls. For more information about keyframing and frame rates: Keyframing in Blender Guide: How to calculate correct frame rate for a sensor Run script to initialize Limulator plugin \u00b6 Click on the scripting window, click on open, and go to the directory which has your source code saved i.e main2.py open it, and press run . Additionally, you can also see the progress of the script in the system console. This is an optional step. To open the system console, Go to windows > toggle system console, this will open a new window of the system console. Now that your script is running go back to the layout page and click on the arrow situated near the viewpoint arrows (x,y, and z direction arrows). You will notice vertical options such as Tool , View , and Limulator . Here you would see three options under limulator dropdown Blender Scene options : This gives option to check and create label IDs for each class of objects in the scene. Create Platform : This lets us create a platform and setup sensors and cameras. Along with this it lets to test and run the simulations. Bulk JSON Process : This option lets us to run multiple simulataneous simualations at a time. Creating a new platform \u00b6 Head to Create platform and open the dropdown menu click on Create new platform , and a new pop-up will show up refer to the image below:. For this tutorial we will add one sensor and one camera. Proceed by adding path to the directory output field, this is important as the JSON file and simulation results will be stored in the specified directory. Once the directory is specified click on Ok . Below new fields will be added to the Create platform dropdown Platform Sensors Test Platform Set platform parameters: \u00b6 Now click on platform from the dropdown, and click on platform parameters . This will lead to yet another pop-up. Enter the base orientation value of the platform with respect to the empty platform object. Go to the movement source and select Blender Object and select the empty platform object created before. Add values to render start as 0 and render end as 160 and click Ok . Refer to the image below. Setup Sensors \u00b6 Now since the platform is ready, its time to setup a LiDAR sensor. Under Sensor dropdown option with the camera ID (1 in current case) is displayed (for example in case of 2 sensors, two options - 1, 2 would be displayed). Clicking the option would display a window to enter sensor parameters. Here, provide location (x, y, z) and orientation (omega, phi, kappa) with respect to the platform object. Remaining parameters like PRF, Scanning frequency, Channels, etc., are specific to each LiDAR sensor and be referred from their data sheets. For more information about sensor parameters: Guide: Understanding sensor parameters Make sure you click Ok after filling all the fields. Please ensure that Ok is clicked on every opened pop-up window. Closing one pop-up window hides other windows as well, thus we may have to open them again and click Ok to proceed. Setup Cameras \u00b6 Configuring a camera sensor is similar to setting up a lidar sensor with camera-specific parameters like focal length, iso, etc. Test Platform \u00b6 Now to visualise how the sensors and cameras would be attached to the platform, head to the Test Platform and click on Visualize . Limulate \u00b6 Press Limulate to start the simulation. This is where the magic happens. The remaining part is straightforward. A laser ray is created at each frame for every channel of the sensor. Intersection of these laser rays with model objects is computed using ray casting. If the calculated distance exceeds the sensor max range, it is discarded; otherwise, we label the attribute and store the location of the intersection point in our point cloud. A csv file is created at the directory you specified earlier, you can visualize the output in Cloud Compare software.","title":"Platform and Sensor setup"},{"location":"tutorials/platform_sensor_setup/#platform-and-sensor-setup","text":"Now that the scene is generated, the next step is to create a platform with the desired sensors and cameras. For that we must specify platform objects and what trajectory they would follow. Platform objects are the parent objects where sensors are embedded, e.g. vehicles, drones, etc for scanning purposes. It could be a static object (with no trajectory) or a dynamic object with a trajectory imported from outside, or an actor(a dynamic object inside the scene). First things first a platform object with a trajectory is to be created. Perform the following steps to create a platform object :","title":"Platform and Sensor Setup"},{"location":"tutorials/platform_sensor_setup/#prepare-scene-collection","text":"Once you are on the home screen of the blender rename the collection as Input and remove the camera and light hierarchy from it. Add a new collection to root i.e scene collection and name it Sensor . The panel should look like this: Collection name is case-sensitive. Make sure your collections name starts with a capital letter, otherwise you\u2019ll encounter a logical error during script execution.","title":"Prepare Scene Collection"},{"location":"tutorials/platform_sensor_setup/#create-a-platform-object","text":"Platform objects can be controlled by either of 2 ways: External trajectories Keyframing in Blender For this tutorial we will create a platform object through keyframing: More Information on use of external trajectories Guide: Using external trajectories Create a new empty object. Go to Add > Empty > Plain axis Create a new collection inside Input collection and rename it to Cars Move the empty object into Cars collection Now if needed we can add some dynamic movement to the platform. This step consists of keyframing which adds motion to the empty object by providing location and orientation values. Lift the editor above to see the timeline and editor controls. For more information about keyframing and frame rates: Keyframing in Blender Guide: How to calculate correct frame rate for a sensor","title":"Create a platform object"},{"location":"tutorials/platform_sensor_setup/#run-script-to-initialize-limulator-plugin","text":"Click on the scripting window, click on open, and go to the directory which has your source code saved i.e main2.py open it, and press run . Additionally, you can also see the progress of the script in the system console. This is an optional step. To open the system console, Go to windows > toggle system console, this will open a new window of the system console. Now that your script is running go back to the layout page and click on the arrow situated near the viewpoint arrows (x,y, and z direction arrows). You will notice vertical options such as Tool , View , and Limulator . Here you would see three options under limulator dropdown Blender Scene options : This gives option to check and create label IDs for each class of objects in the scene. Create Platform : This lets us create a platform and setup sensors and cameras. Along with this it lets to test and run the simulations. Bulk JSON Process : This option lets us to run multiple simulataneous simualations at a time.","title":"Run script to initialize Limulator plugin"},{"location":"tutorials/platform_sensor_setup/#creating-a-new-platform","text":"Head to Create platform and open the dropdown menu click on Create new platform , and a new pop-up will show up refer to the image below:. For this tutorial we will add one sensor and one camera. Proceed by adding path to the directory output field, this is important as the JSON file and simulation results will be stored in the specified directory. Once the directory is specified click on Ok . Below new fields will be added to the Create platform dropdown Platform Sensors Test Platform","title":"Creating a new platform"},{"location":"tutorials/platform_sensor_setup/#set-platform-parameters","text":"Now click on platform from the dropdown, and click on platform parameters . This will lead to yet another pop-up. Enter the base orientation value of the platform with respect to the empty platform object. Go to the movement source and select Blender Object and select the empty platform object created before. Add values to render start as 0 and render end as 160 and click Ok . Refer to the image below.","title":"Set platform parameters:"},{"location":"tutorials/platform_sensor_setup/#setup-sensors","text":"Now since the platform is ready, its time to setup a LiDAR sensor. Under Sensor dropdown option with the camera ID (1 in current case) is displayed (for example in case of 2 sensors, two options - 1, 2 would be displayed). Clicking the option would display a window to enter sensor parameters. Here, provide location (x, y, z) and orientation (omega, phi, kappa) with respect to the platform object. Remaining parameters like PRF, Scanning frequency, Channels, etc., are specific to each LiDAR sensor and be referred from their data sheets. For more information about sensor parameters: Guide: Understanding sensor parameters Make sure you click Ok after filling all the fields. Please ensure that Ok is clicked on every opened pop-up window. Closing one pop-up window hides other windows as well, thus we may have to open them again and click Ok to proceed.","title":"Setup Sensors"},{"location":"tutorials/platform_sensor_setup/#setup-cameras","text":"Configuring a camera sensor is similar to setting up a lidar sensor with camera-specific parameters like focal length, iso, etc.","title":"Setup Cameras"},{"location":"tutorials/platform_sensor_setup/#test-platform","text":"Now to visualise how the sensors and cameras would be attached to the platform, head to the Test Platform and click on Visualize .","title":"Test Platform"},{"location":"tutorials/platform_sensor_setup/#limulate","text":"Press Limulate to start the simulation. This is where the magic happens. The remaining part is straightforward. A laser ray is created at each frame for every channel of the sensor. Intersection of these laser rays with model objects is computed using ray casting. If the calculated distance exceeds the sensor max range, it is discarded; otherwise, we label the attribute and store the location of the intersection point in our point cloud. A csv file is created at the directory you specified earlier, you can visualize the output in Cloud Compare software.","title":"Limulate"},{"location":"tutorials/scene_generation/","text":"Scene Generation with Esri City Engine and Blender \u00b6 Using available pre-built 3D elements and other GIS layers generate a static 3D model and export it as a .blend file. After this open the .blend file and add dynamism to the 3d elements. Follow for more details","title":"Scene Generation Pipeline"},{"location":"tutorials/scene_generation/#scene-generation-with-esri-city-engine-and-blender","text":"Using available pre-built 3D elements and other GIS layers generate a static 3D model and export it as a .blend file. After this open the .blend file and add dynamism to the 3d elements. Follow for more details","title":"Scene Generation with Esri City Engine and Blender"}]}